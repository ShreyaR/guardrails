{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails hub install hub://guardrails/similar_to_document --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize text accurately\n",
    "\n",
    ":::note\n",
    "    To download this example as a Jupyter notebook, click [here](https://github.com/ShreyaR/guardrails/blob/main/docs/examples/text_summarization_quality.ipynb).\n",
    ":::\n",
    "\n",
    "In this example, we will use Guardrails in the summarization of a text document. We will check whether the summarized document has a high semantic similarity with the original document.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Summarize a text document and check whether the summarized document has a high semantic similarity with the original document.\n",
    "\n",
    "## Step 0: Setup\n",
    "\n",
    "In order to run this example, you will need to install the `numpy` package. You can do so by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the RAIL Spec\n",
    "\n",
    "Ordinarily, we would create an RAIL spec in a separate file. For the purposes of this example, we will create the spec in this notebook as a string following the RAIL syntax. For more information on RAIL, see the [RAIL documentation](/docs/how_to_guides/rail).  We will also show the same RAIL spec in a code-first format using a Pydantic model.\n",
    "\n",
    "In this RAIL spec, we:\n",
    "\n",
    "1. Create an `output` schema that returns a single key-value pair. The key should be 'summary', and the value should be the summary of the given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's open our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/article1.txt\", \"r\") as file:\n",
    "    document = file.read()\n",
    "    file.seek(0)\n",
    "    content = \"\".join(line.strip() for line in file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can define our RAIL spec either as a XML string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "rail_str = Template(\n",
    "    \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "\n",
    "<output>\n",
    "    <string\n",
    "        name=\"summary\"\n",
    "        description=\"Summarize the given document faithfully.\"\n",
    "        format=\"similar-to-document: {${document}}, 0.60\"\n",
    "        on-fail-similar-to-document=\"filter\" \n",
    "    />\n",
    "</output>\n",
    "\n",
    "<prompt>\n",
    "Summarize the following document:\n",
    "\n",
    "${document}\n",
    "\n",
    "${gr.complete_xml_suffix}\n",
    "</prompt>\n",
    "</rail>\n",
    "\"\"\"\n",
    ").safe_substitute(document=document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a Pydantic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from guardrails.hub import SimilarToDocument\n",
    "\n",
    "prompt = \"\"\"\n",
    "Summarize the following document:\n",
    "\n",
    "${document}\n",
    "\n",
    "${gr.complete_xml_suffix}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DocumentSummary(BaseModel):\n",
    "    summary: str = Field(\n",
    "        description=\"Summarize the given document faithfully.\",\n",
    "        validators=[\n",
    "            SimilarToDocument(document=f\"'{content}'\", threshold=0.60, on_fail=\"filter\")\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::note\n",
    "\n",
    "    In order to ensure the summary is similar to the document, we use `similar-to-document` as the validator. This validator embeds the document and the summary and checks whether the cosine similarity between the two embeddings is above a threshold.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a `Guard` object with the RAIL Spec\n",
    "\n",
    "We create a `gd.Guard` object that will check, validate and correct the output of the LLM. This object:\n",
    "\n",
    "1. Enforces the quality criteria specified in the RAIL spec.\n",
    "2. Takes corrective action when the quality criteria are not met.\n",
    "3. Compiles the schema and type info from the RAIL spec and adds it to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "import guardrails as gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our RAIL string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.for_rail_string(rail_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from our Pydantic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard.for_pydantic(output_class=DocumentSummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `statement_to_be_translated` is the the statement and will be provided by the user at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Wrap the LLM API call with `Guard`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try translating a statement that doesn't have any profanity in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OPENAI_API_KEY as an environment variable\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "raw_llm_response, validated_response, *rest = guard(\n",
    "    messages=[{\"role\":\"user\", \"content\": prompt}],\n",
    "    prompt_params={\"document\": document},\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"Validated Output: {validated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the prompt that was sent to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.iterations.last.inputs.msg_history[0][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see a detailed look into the logs of the `Guard` object, we can print the `Guard` state history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `guard` wrapper returns the raw_llm_respose (which is a simple string), and the validated and corrected output (which is a dictionary). We can see that the output is a dictionary with the correct schema and types.\n",
    "\n",
    "Next, let's try using a smaller model, which is not going to be good at summarization. We can see that the output is filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_llm_response, validated_response, *rest = guard(\n",
    "    messages=[{\"role\":\"user\", \"content\": prompt}],\n",
    "    prompt_params={\"document\": open(\"data/article1.txt\", \"r\").read()},\n",
    "    model=\"babbage-002\",\n",
    "    max_tokens=512,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"Validated Output: {validated_response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the step-wise history of the `Guard` object below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guardrails",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
