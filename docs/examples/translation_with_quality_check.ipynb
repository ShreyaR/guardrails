{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!guardrails hub install hub://brainlogic/high_quality_translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate text with quality checks\n",
    "\n",
    "**Note:**\n",
    "To download this example as a Jupyter notebook, click [here](https://github.com/guardrails-ai/guardrails/blob/main/docs/examples/translation_with_quality_check.ipynb).\n",
    "\n",
    "In this example, we will use Guardrails during the translation of a statement from another language to English. We will check whether the translated statement is likely of high quality.\n",
    "\n",
    "## Objective\n",
    "\n",
    "We want to translate a statement from different languages to English and ensure that the translated statement accurately reflects the original content.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Install the `unbabel-comet` from source:\n",
    "  `pip install git+https://github.com/Unbabel/COMET`\n",
    "- Please accept the model license from:\n",
    "  https://huggingface.co/Unbabel/wmt22-cometkiwi-da\n",
    "- Login into Huggingface Hub using:\n",
    "  huggingface-cli login --token $HUGGINGFACE_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from guardrails import Guard\n",
    "from rich import print\n",
    "from guardrails.hub import HighQualityTranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define a Guard that uses the validator\n",
    "\n",
    "This guard will use the HighQualityTranslation validator to validate some string outputs.\n",
    "\n",
    "\n",
    "We define the prompt and the guard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the given statement into English:\n",
    "\n",
    "${statement_to_be_translated}\n",
    "\"\"\"\n",
    "\n",
    "guard = Guard().use(HighQualityTranslation(on_fail=\"fix\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Wrap the LLM API call with `Guard`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try translating a statement that is relatively easy to translate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OPENAI_API_KEY as an environment variable\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "statement = \"Ich habe keine Ahnung, was ich hier schreiben soll.\"\n",
    "\n",
    "res = guard(\n",
    "    messages=[{\"role\":\"user\", \"content\": prompt}],\n",
    "    prompt_params={\"statement_to_be_translated\": statement},\n",
    "    metadata={\"translation_source\": statement},\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"Raw LLM Output: {res.raw_llm_output}\")\n",
    "print(f\"Validated Output: {res.validated_output}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the logs to see the quality check results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `guard` wrapper returns the raw LLM response, which is the translated statement and also the validated output. In this case, the translated statement was of a good quality (above the threshold of 0.5), so the validated output is the same as the raw LLM response.\n",
    "\n",
    "#### Now, let's test with a really low quality translation, and see how Guardrails handles it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the code snippet\n",
    "statement = \"अरे भाऊ, आज रात्री जोरदार पार्टी मारूया, जमून टाकूया आणि धमाल करूया!\"\n",
    "\n",
    "## Ideal translation from Marathi -> English:\n",
    "#  \"Hey bro, let's have a great party tonight and have fun!\"\n",
    "\n",
    "output = guard.parse(\n",
    "    llm_output=\"It's such a beautiful day, I'm going to the beach.\",  ## here, providing a really bad translation\n",
    "    metadata={\"translation_source\": statement},\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(f\"Raw LLM Output: {output.raw_llm_output}\")\n",
    "print(f\"Validated Output: {output.validated_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the translation quality is really bad, and the `HighQualityTranslation` check failed as the translation quality was below the threshold. The validated response is an empty string.\n",
    "\n",
    "## In this way, you can use Guardrails to ensure that the output of your LLM is of high quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
