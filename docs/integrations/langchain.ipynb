{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Guardrails from LangChain\n",
    "\n",
    "You can use Guardrails to add a layer of security around LangChain components. Here's how to use Guardrails with LangChain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "\n",
    "Make sure you have both langchain and guardrails installed. If you don't, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install guardrails-ai\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `RAIL` spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_spec = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "\n",
    "<output>\n",
    "    <object name=\"bank_run\" format=\"length: 2\">\n",
    "        <string\n",
    "            name=\"explanation\"\n",
    "            description=\"A paragraph about what a bank run is.\"\n",
    "            format=\"length: 200 240\"\n",
    "            on-fail-length=\"noop\"\n",
    "        />\n",
    "        <url\n",
    "            name=\"follow_up_url\"\n",
    "            description=\"A web URL where I can read more about bank runs.\"\n",
    "            required=\"true\"\n",
    "            format=\"valid-url\"\n",
    "            on-fail-valid-url=\"filter\"\n",
    "        />\n",
    "    </object>\n",
    "</output>\n",
    "\n",
    "<prompt>\n",
    "Explain what a bank run is in a tweet.\n",
    "\n",
    "@xml_prefix_prompt\n",
    "\n",
    "{output_schema}\n",
    "\n",
    "@json_suffix_prompt_v2_wo_none\n",
    "</prompt>\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `GuardrailsOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "from langchain.output_parsers import GuardrailsOutputParser\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = GuardrailsOutputParser.from_rail_string(rail_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GuardrailsOutputParser` contains a `Guard` object, which can be used to access the prompt and output schema. E.g., here is the compiled prompt that is stored in `GuardrailsOutputParser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_parser.guard.base_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a LangChain `PromptTemplate` from this output parser."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the LLM and get formatted, validated and corrected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(temperature=0)\n",
    "\n",
    "output = model(prompt.format_prompt().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiff-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
